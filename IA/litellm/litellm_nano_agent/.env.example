## ── Provider: scegli UNO dei blocchi sotto ──

## Local GGUF (llama.cpp / Ollama / LM Studio)
# LLM_PROVIDER=local
# LOCAL_BASE_URL=http://localhost:8080/v1
# LOCAL_MODEL=codellama

## OpenAI
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-...
# LLM_MODEL=gpt-4o

## Anthropic
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-...
# LLM_MODEL=claude-sonnet-4-20250514

## Perplexity
# LLM_PROVIDER=perplexity
# PERPLEXITY_API_KEY=pplx-...
# LLM_MODEL=sonar-pro

## GitHub Copilot
# LLM_PROVIDER=github_copilot
# LLM_MODEL=gpt-4
# LOCAL_BASE_URL=http://not-deeded
# GITHUB_COPILOT_API_KEY=not-needed-for-github-copilot

